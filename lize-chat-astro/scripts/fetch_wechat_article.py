#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–è„šæœ¬
å°†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è½¬æ¢ä¸ºç¬¦åˆ lize.chat é¡¹ç›®æ ¼å¼çš„ Markdown æ–‡ä»¶
"""

import requests
from bs4 import BeautifulSoup
import re
import os
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
import html2text
import json

# é…ç½®
CONTENT_DIR = Path(__file__).parent.parent / "src" / "content" / "blog"
CONTENT_DIR.mkdir(parents=True, exist_ok=True)

# è®¾ç½® User-Agentï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

# åˆå§‹åŒ– html2text è½¬æ¢å™¨
h = html2text.HTML2Text()
h.ignore_links = False
h.ignore_images = False
h.body_width = 0  # ä¸é™åˆ¶å®½åº¦
h.unicode_snob = True  # ä½¿ç”¨ Unicode å­—ç¬¦


def sanitize_filename(title):
    """å°†æ ‡é¢˜è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€è¿å­—ç¬¦å’Œä¸‹åˆ’çº¿
    filename = re.sub(r'[^\w\s-]', '', title)
    # å°†ç©ºæ ¼æ›¿æ¢ä¸ºè¿å­—ç¬¦
    filename = re.sub(r'\s+', '-', filename)
    # é™åˆ¶é•¿åº¦
    filename = filename[:100]
    return filename


def extract_wechat_article(url):
    """
    æå–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹
    æ³¨æ„ï¼šå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é€šå¸¸éœ€è¦ç‰¹æ®Šè®¿é—®æ–¹å¼ï¼Œæ­¤è„šæœ¬å¤„ç†å¯ç›´æ¥è®¿é—®çš„ URL
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_selectors = [
            'h1#activity-name',
            'h1.rich_media_title',
            'h1',
            'title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break
        
        if not title:
            title = soup.find('title')
            if title:
                title = title.get_text(strip=True)
                # ç§»é™¤å¸¸è§çš„åç¼€
                title = re.sub(r'\s*-\s*.*$', '', title)
        
        # æå–å‘å¸ƒæ—¥æœŸ
        pub_date = None
        date_selectors = [
            '#publish_time',
            '.publish_time',
            'em#publish_time',
            'em.rich_media_meta_text',
        ]
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                # å°è¯•è§£ææ—¥æœŸ
                try:
                    # å¾®ä¿¡å…¬ä¼—å·æ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "2024-01-15" æˆ– "2024å¹´1æœˆ15æ—¥"
                    date_match = re.search(r'(\d{4})[å¹´\-/](\d{1,2})[æœˆ\-/](\d{1,2})', date_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        pub_date = datetime(int(year), int(month), int(day))
                        break
                except:
                    pass
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if not pub_date:
            pub_date = datetime.now()
        
        # æå–æ­£æ–‡å†…å®¹
        content = None
        content_selectors = [
            '#js_content',
            '.rich_media_content',
            'div[class*="content"]',
            'article',
        ]
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem
                break
        
        if not content:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„ div
            content = soup.find('div', class_=re.compile(r'content|article|text'))
        
        if not content:
            raise ValueError("æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹")
        
        # è½¬æ¢ä¸º Markdown
        markdown_content = h.handle(str(content))
        
        # æ¸…ç† Markdown å†…å®¹
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        markdown_content = markdown_content.strip()
        
        return {
            'title': title or 'æœªå‘½åæ–‡ç« ',
            'date': pub_date,
            'content': markdown_content,
            'url': url
        }
        
    except Exception as e:
        raise Exception(f"æŠ“å–æ–‡ç« å¤±è´¥: {str(e)}")


def generate_frontmatter(title, date, description=None, guest=None, host=None, slide_url=None):
    """ç”Ÿæˆ Frontmatter"""
    frontmatter = {
        'title': title,
        'pubDate': date.strftime('%Y-%m-%d'),
    }
    
    if description:
        frontmatter['description'] = description
    
    if guest:
        frontmatter['guest'] = guest
    
    if host:
        frontmatter['host'] = host
    
    if slide_url:
        frontmatter['slideUrl'] = slide_url
    
    # æ ¼å¼åŒ–ä¸º YAML
    lines = ['---']
    for key, value in frontmatter.items():
        if isinstance(value, str):
            # å¦‚æœå€¼åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿ç”¨å¼•å·
            if ':' in value or '"' in value or "'" in value:
                lines.append(f'{key}: "{value}"')
            else:
                lines.append(f'{key}: {value}')
        else:
            lines.append(f'{key}: {value}')
    lines.append('---')
    
    return '\n'.join(lines)


def save_article(article_data, output_dir=None):
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    if output_dir is None:
        output_dir = CONTENT_DIR
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = sanitize_filename(article_data['title'])
    filepath = output_dir / f"{filename}.md"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
    if filepath.exists():
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filepath = output_dir / f"{filename}_{timestamp}.md"
    
    # ç”Ÿæˆæ‘˜è¦ï¼ˆå–å‰100ä¸ªå­—ç¬¦ï¼‰
    description = None
    if article_data['content']:
        # ç§»é™¤ Markdown æ ¼å¼æ ‡è®°ï¼Œæå–çº¯æ–‡æœ¬
        text_content = re.sub(r'[#*_\[\]()]', '', article_data['content'])
        text_content = re.sub(r'\n+', ' ', text_content)
        description = text_content[:100].strip()
        if len(text_content) > 100:
            description += '...'
    
    # ç”Ÿæˆ Frontmatter
    frontmatter = generate_frontmatter(
        title=article_data['title'],
        date=article_data['date'],
        description=description
    )
    
    # ç»„åˆå†…å®¹
    full_content = f"{frontmatter}\n\n{article_data['content']}"
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    print(f"âœ… æ–‡ç« å·²ä¿å­˜: {filepath}")
    return filepath


def process_urls(urls):
    """æ‰¹é‡å¤„ç†å¤šä¸ª URL"""
    results = []
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†: {url}")
        try:
            article_data = extract_wechat_article(url)
            filepath = save_article(article_data)
            results.append({
                'url': url,
                'success': True,
                'filepath': str(filepath),
                'title': article_data['title']
            })
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
            results.append({
                'url': url,
                'success': False,
                'error': str(e)
            })
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python fetch_wechat_article.py <URL1> [URL2] [URL3] ...")
        print("\næˆ–è€…ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨:")
        print("  python fetch_wechat_article.py --file urls.txt")
        print("\nç¤ºä¾‹:")
        print("  python fetch_wechat_article.py https://mp.weixin.qq.com/s/xxxxx")
        return
    
    urls = []
    
    # æ£€æŸ¥æ˜¯å¦ä»æ–‡ä»¶è¯»å–
    if sys.argv[1] == '--file' and len(sys.argv) > 2:
        filepath = sys.argv[2]
        with open(filepath, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    else:
        urls = sys.argv[1:]
    
    if not urls:
        print("âŒ æ²¡æœ‰æä¾›æœ‰æ•ˆçš„ URL")
        return
    
    print(f"ğŸ“ å‡†å¤‡å¤„ç† {len(urls)} ç¯‡æ–‡ç« ...")
    results = process_urls(urls)
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*50)
    print("å¤„ç†å®Œæˆï¼")
    print("="*50)
    success_count = sum(1 for r in results if r['success'])
    print(f"æˆåŠŸ: {success_count}/{len(results)}")
    
    if success_count > 0:
        print("\næˆåŠŸä¿å­˜çš„æ–‡ç« :")
        for r in results:
            if r['success']:
                print(f"  - {r['title']}")
                print(f"    æ–‡ä»¶: {r['filepath']}")


if __name__ == '__main__':
    main()
