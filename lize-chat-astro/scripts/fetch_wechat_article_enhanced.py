#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–è„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
æ”¯æŒæ›´å¤šå†…å®¹æå–é€‰é¡¹å’Œè‡ªå®šä¹‰é…ç½®
"""

import requests
from bs4 import BeautifulSoup
import re
import os
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse, urljoin
import html2text
import json
import argparse

# é…ç½®
CONTENT_DIR = Path(__file__).parent.parent / "src" / "content" / "blog"
CONTENT_DIR.mkdir(parents=True, exist_ok=True)

# è®¾ç½® User-Agent
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

# åˆå§‹åŒ– html2text è½¬æ¢å™¨
h = html2text.HTML2Text()
h.ignore_links = False
h.ignore_images = True  # å¿½ç•¥å›¾ç‰‡ï¼Œå› ä¸ºå¾®ä¿¡å…¬ä¼—å·å›¾ç‰‡é€šå¸¸éœ€è¦ç‰¹æ®Šå¤„ç†
h.body_width = 0
h.unicode_snob = True
h.mark_code = True  # ä¿ç•™ä»£ç å—


def sanitize_filename(title):
    """å°†æ ‡é¢˜è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
    filename = re.sub(r'[^\w\s-]', '', title)
    filename = re.sub(r'\s+', '-', filename)
    filename = filename[:100]
    return filename


def parse_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼"""
    if not date_str:
        return None
    
    # å¸¸è§æ—¥æœŸæ ¼å¼
    patterns = [
        (r'(\d{4})[å¹´\-/](\d{1,2})[æœˆ\-/](\d{1,2})[æ—¥]?', '%Y-%m-%d'),
        (r'(\d{4})-(\d{2})-(\d{2})', '%Y-%m-%d'),
        (r'(\d{4})/(\d{2})/(\d{2})', '%Y-%m-%d'),
    ]
    
    for pattern, _ in patterns:
        match = re.search(pattern, date_str)
        if match:
            try:
                year, month, day = match.groups()
                return datetime(int(year), int(month), int(day))
            except:
                continue
    
    return None


def extract_wechat_article(url, extract_images=False):
    """
    æå–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = None
        title_selectors = [
            'h1#activity-name',
            'h1.rich_media_title',
            'h1.rich_media_title#activity-name',
            'h1',
            'title'
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                # ç§»é™¤å¸¸è§çš„åç¼€
                title = re.sub(r'\s*[-|]\s*.*å¾®ä¿¡å…¬ä¼—å·.*$', '', title, flags=re.IGNORECASE)
                title = re.sub(r'\s*[-|]\s*.*$', '', title)
                if title:
                    break
        
        if not title:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                title = re.sub(r'\s*[-|]\s*.*$', '', title)
        
        if not title:
            title = 'æœªå‘½åæ–‡ç« '
        
        # æå–å‘å¸ƒæ—¥æœŸ
        pub_date = None
        date_selectors = [
            '#publish_time',
            '.publish_time',
            'em#publish_time',
            'em.rich_media_meta_text',
            'span#publish_time',
            'div.rich_media_meta_text',
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                pub_date = parse_date(date_text)
                if pub_date:
                    break
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä» meta æ ‡ç­¾è·å–
        if not pub_date:
            meta_date = soup.find('meta', property='article:published_time')
            if meta_date:
                date_str = meta_date.get('content', '')
                try:
                    pub_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except:
                    pass
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
        if not pub_date:
            pub_date = datetime.now()
        
        # æå–æ­£æ–‡å†…å®¹
        content = None
        content_selectors = [
            '#js_content',
            '.rich_media_content',
            'div[id*="content"]',
            'div[class*="content"]',
            'article',
            'div.article-content',
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem
                break
        
        if not content:
            # å°è¯•æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬çš„ div
            divs = soup.find_all('div', class_=re.compile(r'content|article|text|rich'))
            for div in divs:
                text_length = len(div.get_text())
                if text_length > 500:  # å‡è®¾æ­£æ–‡è‡³å°‘500å­—ç¬¦
                    content = div
                    break
        
        if not content:
            raise ValueError("æ— æ³•æ‰¾åˆ°æ–‡ç« å†…å®¹")
        
        # æ¸…ç†ä¸éœ€è¦çš„å…ƒç´ 
        for elem in content.find_all(['script', 'style', 'iframe', 'noscript']):
            elem.decompose()
        
        # ç§»é™¤å¾®ä¿¡å…¬ä¼—å·ç‰¹æœ‰çš„å…ƒç´ 
        for elem in content.find_all(class_=re.compile(r'qr|code|ad|advertisement|promotion')):
            elem.decompose()
        
        # è½¬æ¢ä¸º Markdown
        markdown_content = h.handle(str(content))
        
        # æ¸…ç† Markdown å†…å®¹
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        # ç§»é™¤é¦–å°¾ç©ºç™½
        markdown_content = markdown_content.strip()
        # ç§»é™¤å¾®ä¿¡å…¬ä¼—å·äºŒç»´ç ç­‰æç¤º
        markdown_content = re.sub(r'é•¿æŒ‰.*å…³æ³¨.*\n?', '', markdown_content, flags=re.IGNORECASE)
        markdown_content = re.sub(r'æ‰«ç .*å…³æ³¨.*\n?', '', markdown_content, flags=re.IGNORECASE)
        
        return {
            'title': title,
            'date': pub_date,
            'content': markdown_content,
            'url': url
        }
        
    except requests.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except Exception as e:
        raise Exception(f"æå–æ–‡ç« å¤±è´¥: {str(e)}")


def generate_frontmatter(title, date, description=None, guest=None, host=None, slide_url=None, tags=None):
    """ç”Ÿæˆ Frontmatter"""
    frontmatter = {
        'title': f'"{title}"',  # æ ‡é¢˜ç”¨å¼•å·åŒ…è£¹ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
        'pubDate': date.strftime('%Y-%m-%d'),
    }
    
    if description:
        frontmatter['description'] = f'"{description}"'
    
    if guest:
        frontmatter['guest'] = f'"{guest}"'
    
    if host:
        frontmatter['host'] = f'"{host}"'
    
    if slide_url:
        frontmatter['slideUrl'] = f'"{slide_url}"'
    
    if tags:
        if isinstance(tags, list):
            tags_str = '[' + ', '.join([f'"{tag}"' for tag in tags]) + ']'
            frontmatter['tags'] = tags_str
        else:
            frontmatter['tags'] = f'"{tags}"'
    
    # æ ¼å¼åŒ–ä¸º YAML
    lines = ['---']
    for key, value in frontmatter.items():
        lines.append(f'{key}: {value}')
    lines.append('---')
    
    return '\n'.join(lines)


def extract_summary(content, max_length=150):
    """ä»å†…å®¹ä¸­æå–æ‘˜è¦"""
    if not content:
        return None
    
    # ç§»é™¤ Markdown æ ¼å¼æ ‡è®°
    text = re.sub(r'[#*_\[\]()]', '', content)
    text = re.sub(r'\n+', ' ', text)
    text = text.strip()
    
    # å–å‰ max_length ä¸ªå­—ç¬¦
    if len(text) > max_length:
        # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text[:max_length * 2])
        if len(sentences) > 1:
            summary = 'ã€‚'.join(sentences[:-1]) + 'ã€‚'
        else:
            summary = text[:max_length] + '...'
    else:
        summary = text
    
    return summary


def save_article(article_data, output_dir=None, guest=None, host=None, tags=None):
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    if output_dir is None:
        output_dir = CONTENT_DIR
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = sanitize_filename(article_data['title'])
    filepath = output_dir / f"{filename}.md"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
    counter = 1
    original_filepath = filepath
    while filepath.exists():
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filepath = output_dir / f"{filename}_{timestamp}_{counter}.md"
        counter += 1
    
    # ç”Ÿæˆæ‘˜è¦
    description = extract_summary(article_data['content'])
    
    # ç”Ÿæˆ Frontmatter
    frontmatter = generate_frontmatter(
        title=article_data['title'],
        date=article_data['date'],
        description=description,
        guest=guest,
        host=host,
        tags=tags
    )
    
    # ç»„åˆå†…å®¹
    full_content = f"{frontmatter}\n\n{article_data['content']}"
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(full_content)
    
    return filepath


def main():
    parser = argparse.ArgumentParser(description='æŠ“å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å¹¶è½¬æ¢ä¸º Markdown')
    parser.add_argument('urls', nargs='*', help='å¾®ä¿¡å…¬ä¼—å·æ–‡ç«  URL')
    parser.add_argument('--file', '-f', help='ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨')
    parser.add_argument('--guest', '-g', help='å˜‰å®¾åç§°')
    parser.add_argument('--host', '-h', help='ä¸»æŒäººåç§°ï¼ˆé»˜è®¤ä¸º"ä¸½æ³½"ï¼‰', default='ä¸½æ³½')
    parser.add_argument('--tags', '-t', help='æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”', default='')
    parser.add_argument('--output', '-o', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸º src/content/blogï¼‰')
    
    args = parser.parse_args()
    
    # è·å– URL åˆ—è¡¨
    urls = []
    if args.file:
        with open(args.file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    else:
        urls = args.urls
    
    if not urls:
        parser.print_help()
        return
    
    # è§£ææ ‡ç­¾
    tags = [tag.strip() for tag in args.tags.split(',')] if args.tags else None
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output) if args.output else CONTENT_DIR
    
    print(f"ğŸ“ å‡†å¤‡å¤„ç† {len(urls)} ç¯‡æ–‡ç« ...")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    results = []
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†: {url}")
        try:
            article_data = extract_wechat_article(url)
            filepath = save_article(
                article_data,
                output_dir=output_dir,
                guest=args.guest,
                host=args.host,
                tags=tags
            )
            results.append({
                'url': url,
                'success': True,
                'filepath': str(filepath),
                'title': article_data['title']
            })
            print(f"âœ… æˆåŠŸ: {article_data['title']}")
            print(f"   ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"âŒ å¤±è´¥: {str(e)}")
            results.append({
                'url': url,
                'success': False,
                'error': str(e)
            })
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("å¤„ç†å®Œæˆï¼")
    print("="*60)
    success_count = sum(1 for r in results if r['success'])
    print(f"æˆåŠŸ: {success_count}/{len(results)}")
    
    if success_count > 0:
        print("\næˆåŠŸä¿å­˜çš„æ–‡ç« :")
        for r in results:
            if r['success']:
                print(f"  âœ“ {r['title']}")
                print(f"    {r['filepath']}")


if __name__ == '__main__':
    main()
